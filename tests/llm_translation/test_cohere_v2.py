import os
import sys
import traceback
from unittest.mock import AsyncMock, MagicMock, patch

from dotenv import load_dotenv

# Load test environment variables
load_dotenv(dotenv_path=".env.test")
import io
import os

sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path
import json
import time
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
import httpx

import litellm
from litellm import completion
from litellm.caching import DualCache
from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler, HTTPHandler
from litellm.utils import ModelResponse, ProviderConfigManager

litellm.num_retries = 3

# Mock response for Cohere v2 API
def get_mock_response(stream=False, with_tool_calls=False, with_citations=False):
    if with_tool_calls:
        # Format the response to match the expected OpenAI-compatible format
        # The transformation_v2.py will convert this to the OpenAI format with tool_calls
        content = {
            "response_id": "123",
            "text": "The weather in Boston is currently 72째F.",
            "generation_id": "gen_123",
            "tool_calls": [
                {
                    "id": "call_0",
                    "type": "function",
                    "function": {
                        "name": "get_current_weather",
                        "arguments": '{"location":"Boston, MA","unit":"fahrenheit"}'
                    }
                }
            ],
            "citations": [],
            "meta": {
                "api_version": {"version": "1"},
                "billed_units": {
                    "input_tokens": 15,
                    "output_tokens": 25
                }
            }
        }
    elif with_citations:
        content = {
            "response_id": "123",
            "text": "Emperor penguins are the tallest species of penguin.",
            "generation_id": "gen_123",
            "tool_calls": [],
            "citations": [
                {
                    "start": 0,
                    "end": 15,
                    "text": "Emperor penguins",
                    "document_ids": ["doc1"],
                    "url": ""
                }
            ],
            "meta": {
                "api_version": {"version": "1"},
                "billed_units": {
                    "input_tokens": 12,
                    "output_tokens": 18
                }
            }
        }
    else:
        content = {
            "response_id": "123",
            "text": "Hello! How can I help you today?",
            "generation_id": "gen_123",
            "tool_calls": [],
            "citations": [],
            "meta": {
                "api_version": {"version": "1"},
                "billed_units": {
                    "input_tokens": 10,
                    "output_tokens": 20
                }
            }
        }
    
    mock_response = httpx.Response(200, json=content)
    return mock_response


@pytest.mark.parametrize("stream", [True, False])
@pytest.mark.flaky(retries=3, delay=1)
@patch("litellm.llms.cohere.chat.transformation_v2.cohere_validate_environment")
@patch.object(HTTPHandler, "post")
@patch.object(AsyncHTTPHandler, "post")
def test_basic_chat_completion_cohere_v2(async_post_mock, post_mock, mock_validate_env, stream):
    # Mock the validation function to return a valid API key
    mock_validate_env.return_value = {"api_key": "test-api-key-for-cohere-v2"}
    
    # Set up environment variable
    os.environ["COHERE_API_KEY"] = "test-api-key-for-cohere-v2"
    """
    Test for basic non-streaming and streaming requests with Cohere v2 API
    Success criteria: Basic non-streaming and streaming requests should work with Cohere v2 API
    """
    # Setup mocks
    mock_response = get_mock_response(stream=stream)
    
    if stream:
        # For streaming, we need to mock the iter_lines method
        mock_stream = MagicMock()
        mock_stream.iter_lines.return_value = iter([json.dumps({
            "response_id": "123",
            "text": "Hello! How can I assist you today?",
            "generation_id": "gen_123",
            "citations": [],
            "meta": {
                "api_version": {"version": "1"},
                "billed_units": {
                    "input_tokens": 10,
                    "output_tokens": 20
                }
            }
        }).encode()])
        post_mock.return_value = mock_stream 
    else:
        post_mock.return_value = mock_response
    
    try:
        litellm.set_verbose = True
        messages = [
            {"role": "system", "content": "You're a helpful assistant"},
            {"role": "assistant", "content": [{"text": "2", "type": "text"}]},
            {"role": "assistant", "content": [{"text": "3", "type": "text"}]},
            {
                "role": "user",
                "content": "Hey",
            },
        ]
        response = completion(
            model="cohere_chat_v2/command-r",
            messages=messages,
            stream=stream,
        )
        
        if stream:
            # Test streaming response
            content = ""
            for chunk in response:
                if hasattr(chunk.choices[0].delta, "content") and chunk.choices[0].delta.content is not None:
                    content += chunk.choices[0].delta.content
            assert len(content) > 0
        else:
            # Test non-streaming response
            assert response.choices[0].message.content is not None
            assert len(response.choices[0].message.content) > 0
            assert response.usage.prompt_tokens == 10
            assert response.usage.completion_tokens == 20
            
        print(f"Cohere v2 {'streaming' if stream else 'non-streaming'} test passed")
    except Exception as e:
        pytest.fail(f"Error occurred: {e}")


@pytest.mark.parametrize("stream", [True, False])
@pytest.mark.flaky(retries=3, delay=1)
@pytest.mark.asyncio
@patch("litellm.llms.cohere.chat.transformation_v2.cohere_validate_environment")
@patch.object(AsyncHTTPHandler, "post")
async def test_async_chat_completion_cohere_v2(async_post_mock, mock_validate_env, stream):
    # Mock the validation function to return a valid API key
    mock_validate_env.return_value = {"api_key": "test-api-key-for-cohere-v2"}
    
    # Set up environment variable
    os.environ["COHERE_API_KEY"] = "test-api-key-for-cohere-v2"
    """
    Test for async basic non-streaming and streaming requests with Cohere v2 API
    Success criteria: Basic non-streaming and streaming requests should work with Cohere v2 API
    """
    # Setup mock
    mock_response = get_mock_response(stream=stream)
    async_post_mock.return_value = mock_response
    
    try:
        litellm.set_verbose = True
        messages = [
            {"role": "system", "content": "You're a helpful assistant"},
            {
                "role": "user",
                "content": "Hey",
            },
        ]
        response = await litellm.acompletion(
            model="cohere_chat_v2/command-r",
            messages=messages,
            max_tokens=10,
            stream=stream,
        )
        
        if stream:
            # Test streaming response
            content = ""
            async for chunk in response:
                if hasattr(chunk.choices[0].delta, "content") and chunk.choices[0].delta.content is not None:
                    content += chunk.choices[0].delta.content
            assert len(content) > 0
        else:
            # Test non-streaming response
            assert response.choices[0].message.content is not None
            assert len(response.choices[0].message.content) > 0
            assert response.usage.prompt_tokens == 10
            assert response.usage.completion_tokens == 20
            
        print(f"Cohere v2 async {'streaming' if stream else 'non-streaming'} test passed")
    except Exception as e:
        pytest.fail(f"Error occurred: {e}")


@pytest.mark.parametrize("stream", [True, False])
@pytest.mark.flaky(retries=3, delay=1)
@pytest.mark.asyncio
@patch("litellm.llms.cohere.chat.transformation_v2.cohere_validate_environment")
@patch.object(AsyncHTTPHandler, "post")
async def test_chat_completion_cohere_v2_citations(async_post_mock, mock_validate_env, stream):
    # Mock the validation function to return a valid API key
    mock_validate_env.return_value = {"api_key": "test-api-key-for-cohere-v2"}
    
    # Set up environment variable
    os.environ["COHERE_API_KEY"] = "test-api-key-for-cohere-v2"
    """
    Test for citations with Cohere v2 API
    Success criteria: Citations should work with both streaming and non-streaming requests
    """
    # Create a mock response with citations
    mock_response = get_mock_response(stream=stream, with_citations=True)
    async_post_mock.return_value = mock_response
    
    try:
        litellm.set_verbose = True
        messages = [
            {
                "role": "user",
                "content": "Which penguins are the tallest?",
            },
        ]
        response = await litellm.acompletion(
            model="cohere_chat_v2/command-r",
            messages=messages,
            documents=[
                {"title": "Tall penguins", "text": "Emperor penguins are the tallest."},
                {
                    "title": "Penguin habitats",
                    "text": "Emperor penguins only live in Antarctica.",
                },
            ],
            stream=stream,
        )

        if stream:
            citations_chunk = False
            async for chunk in response:
                print("received chunk", chunk)
                if hasattr(chunk, "citations"):
                    citations_chunk = True
                    break
            assert citations_chunk
        else:
            assert hasattr(response, "citations")
            assert response.citations is not None
            assert response.citations[0]["text"] == "Emperor penguins"
            assert response.usage.prompt_tokens == 12
            assert response.usage.completion_tokens == 18
            
        print(f"Cohere v2 citations {'streaming' if stream else 'non-streaming'} test passed")
    except litellm.ServiceUnavailableError:
        pass
    except Exception as e:
        pytest.fail(f"Error occurred: {e}")


def test_completion_cohere_v2_function_call():
    """
    Test for tool calling response transformation with Cohere v2 API
    Success criteria: Tool calls in Cohere v2 API response should be properly transformed to OpenAI format
    """
    # Import the class directly to test
    from litellm.llms.cohere.chat.transformation_v2 import CohereChatConfigV2
    
    # Create an instance of the config class
    config = CohereChatConfigV2()
    
    # Create a mock Cohere v2 API response with tool calls
    cohere_response = httpx.Response(200, json={
        "response_id": "123",
        "text": "The weather in Boston is currently 72째F.",
        "generation_id": "gen_123",
        "tool_calls": [
            {
                "name": "get_current_weather",
                "parameters": {
                    "location": "Boston, MA",
                    "unit": "fahrenheit"
                }
            }
        ],
        "citations": [],
        "meta": {
            "api_version": {"version": "1"},
            "billed_units": {
                "input_tokens": 15,
                "output_tokens": 25
            }
        }
    })
    
    # Create a basic ModelResponse object to be populated by transform_response
    model_response = litellm.ModelResponse(
        id="chatcmpl-123",
        choices=[{
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "",
                "role": "assistant"
            }
        }],
        created=int(time.time()),
        model="command-r-plus",
        object="chat.completion",
        usage={
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    )
    
    # Define test messages and parameters
    messages = [
        {
            "role": "user",
            "content": "What's the weather like in Boston today in Fahrenheit?",
        }
    ]
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Get the current weather in a given location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                    },
                    "required": ["location"],
                },
            },
        }
    ]
    
    # Create a mock logging object
    logging_obj = MagicMock()
    
    # Create request data
    request_data = {
        "message": "What's the weather like in Boston today in Fahrenheit?",
        "tools": tools
    }
    
    try:
        # Call transform_response directly
        transformed_response = config.transform_response(
            model="command-r-plus",
            raw_response=cohere_response,
            model_response=model_response,
            logging_obj=logging_obj,
            request_data=request_data,
            messages=messages,
            optional_params={"tools": tools},
            litellm_params={},
            encoding=None,
            api_key="test-api-key"
        )
        
        # Verify the response transformation
        print(transformed_response)
        
        # Verify that tool calls were properly transformed
        assert hasattr(transformed_response.choices[0].message, "tool_calls")
        assert len(transformed_response.choices[0].message.tool_calls) > 0
        
        # Access the tool call object and verify its properties
        tool_call = transformed_response.choices[0].message.tool_calls[0]
        # Print the tool call object for debugging
        print(f"Tool call object: {tool_call}")
        
        # Check if tool_call is a dictionary or an object
        if isinstance(tool_call, dict):
            assert "function" in tool_call
            assert tool_call["function"]["name"] == "get_current_weather"
        else:
            assert hasattr(tool_call, "function")
            assert tool_call.function.name == "get_current_weather"
        
        # Verify the content was set correctly
        assert transformed_response.choices[0].message.content == "The weather in Boston is currently 72째F."
        
        # Verify usage information
        assert transformed_response.usage.prompt_tokens == 15
        assert transformed_response.usage.completion_tokens == 25
    except Exception as e:
        print(f"Error details: {str(e)}")
        pytest.fail(f"Test failed with exception: {e}")
    
    print("Cohere v2 tool calling response transformation test passed")


def test_completion_cohere_v2_streaming_tool_call():python -m pytest tests/
    """
    Test for streaming tool calling response transformation with Cohere v2 API
    Success criteria: Streaming tool calls in Cohere v2 API response should be properly transformed
    """
    try:
        # Import the class directly to test
        from litellm.llms.cohere.chat.transformation_v2 import CohereChatConfigV2
        from litellm.utils import ModelResponse, Choices, Message, Delta
        
        # Create an instance of the config class
        config = CohereChatConfigV2()
        
        # Create mock streaming chunks that simulate a Cohere v2 API streaming response
        stream_chunks = [
            json.dumps({"text": "The weather", "response_id": "123", "generation_id": "gen_123", "meta": {"api_version": {"version": "1"}, "billed_units": {"input_tokens": 15, "output_tokens": 5}}}),
            json.dumps({"text": " in Boston", "response_id": "123", "generation_id": "gen_123", "meta": {"api_version": {"version": "1"}, "billed_units": {"input_tokens": 15, "output_tokens": 10}}}),
            json.dumps({"text": " is currently", "response_id": "123", "generation_id": "gen_123", "meta": {"api_version": {"version": "1"}, "billed_units": {"input_tokens": 15, "output_tokens": 15}}}),
            json.dumps({"text": " 72째F.", "response_id": "123", "generation_id": "gen_123", "meta": {"api_version": {"version": "1"}, "billed_units": {"input_tokens": 15, "output_tokens": 20}}}),
            json.dumps({"tool_calls": [{"name": "get_current_weather", "parameters": {"location": "Boston, MA", "unit": "fahrenheit"}}], "response_id": "123", "generation_id": "gen_123", "meta": {"api_version": {"version": "1"}, "billed_units": {"input_tokens": 15, "output_tokens": 25}}})
        ]
        
        # Create a mock iterator that will yield the stream chunks
        class MockStreamIterator:
            def __init__(self, chunks):
                self.chunks = chunks
                self.index = 0
            
            def __iter__(self):
                return self
            
            def __next__(self):
                if self.index < len(self.chunks):
                    chunk = self.chunks[self.index]
                    self.index += 1
                    return chunk
                raise StopIteration
        
        # Create the mock iterator
        mock_iterator = MockStreamIterator(stream_chunks)
        
        # Create a ModelResponse object to track the state during streaming
        model_response = ModelResponse(
            id="chatcmpl-123",
            choices=[{
                "finish_reason": None,
                "index": 0,
                "delta": {
                    "content": None,
                    "role": "assistant"
                }
            }],
            created=int(time.time()),
            model="command-r-plus",
            object="chat.completion.chunk",
            usage={
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        )
        
        # Process the stream manually
        chunks = []
        tool_calls_received = False
        content_received = False
        
        for chunk_str in stream_chunks:
            chunk_data = json.loads(chunk_str)
            
            # Create a response chunk
            chunk = ModelResponse(
                id="chatcmpl-123",
                choices=[{
                    "finish_reason": None,
                    "index": 0,
                    "delta": {}
                }],
                created=int(time.time()),
                model="command-r-plus",
                object="chat.completion.chunk"
            )
            
            # Process text content
            if "text" in chunk_data:
                chunk.choices[0]["delta"]["content"] = chunk_data["text"]
                content_received = True
            
            # Process tool calls
            if "tool_calls" in chunk_data:
                tool_calls = []
                for tool_call_data in chunk_data["tool_calls"]:
                    tool_call = {
                        "index": 0,
                        "id": f"call_{len(tool_calls)}",
                        "type": "function",
                        "function": {
                            "name": tool_call_data["name"],
                            "arguments": json.dumps(tool_call_data["parameters"])
                        }
                    }
                    tool_calls.append(tool_call)
                
                chunk.choices[0]["delta"]["tool_calls"] = tool_calls
                tool_calls_received = True
                
                # Verify the tool call details
                assert chunk.choices[0]["delta"]["tool_calls"][0]["function"]["name"] == "get_current_weather"
                args = json.loads(chunk.choices[0]["delta"]["tool_calls"][0]["function"]["arguments"])
                assert "location" in args
                assert "unit" in args
            
            chunks.append(chunk)
        
        # Verify that we received both content and tool calls
        assert content_received, "No content was received in the stream"
        assert tool_calls_received, "No tool calls were received in the stream"
        
        # Verify that we received the expected number of chunks
        assert len(chunks) > 0, "No chunks were received"
        
        print("Cohere v2 streaming tool call response transformation test passed")
    except Exception as e:
        print(f"Error details: {str(e)}")
        pytest.fail(f"Test failed with exception: {e}")


def test_cohere_v2_request_body_format():
    """
    Test to validate that the request body format for Cohere v2 API is correct
    Success criteria: The request body format should match the Cohere v2 API specification
    """
    # Import the class directly to test
    from litellm.llms.cohere.chat.transformation_v2 import CohereChatConfigV2
    
    # Define test parameters
    test_tools = [{
        "type": "function",
        "function": {
            "name": "get_current_time",
            "description": "Get the current time in a given location.",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "The city name, e.g. San Francisco"}
                },
                "required": ["location"]
            }
        }
    }]
    
    # Define the input parameters
    messages = [
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "what time is it in San Francisco?"}
    ]
    
    # Create an instance of the config class
    config = CohereChatConfigV2()
    
    # Call the transform_request method to get the request body
    request_data = config.transform_request(
        model="command-r-plus",
        messages=messages,
        optional_params={
            "tools": test_tools,
            "tool_choice": "auto"
        },
        litellm_params={},
        headers={}
    )
    
    # Print the request data for debugging
    print(f"Request data: {json.dumps(request_data, indent=2)}")
    
    # Verify the request body format matches Cohere v2 API specification
    assert "message" in request_data, "Request body should contain 'message' field"
    assert "tools" in request_data, "Request body should contain 'tools' field"
    assert "chat_history" in request_data, "Request body should contain 'chat_history' field"
    
    # Verify the message format
    assert request_data["message"] == "what time is it in San Francisco?", "Message content is incorrect"
    
    # Verify chat history format
    assert isinstance(request_data["chat_history"], list), "Chat history should be a list"
    assert len(request_data["chat_history"]) == 1, "Chat history should have one message"
    assert request_data["chat_history"][0]["role"] == "system", "First message role should be system"
    assert request_data["chat_history"][0]["message"] == "You are a helpful assistant", "System message content is incorrect"
    
    # Verify the tools format in the request body
    assert isinstance(request_data["tools"], list), "Tools should be a list"
    assert len(request_data["tools"]) > 0, "Tools list should not be empty"
    
    # Verify tool structure
    tool = request_data["tools"][0]
    assert "name" in tool, "Tool should have 'name' field"
    assert "description" in tool, "Tool should have 'description' field"
    assert "parameter_definitions" in tool, "Tool should have 'parameter_definitions' field"
    
    print("Cohere v2 request body format test passed")
